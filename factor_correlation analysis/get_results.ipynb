{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Set up file paths\n",
    "os.chdir('/Users/nsusser/Desktop/Github/happyDB/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_path = 'dataframes/clean_sentences.csv'\n",
    "items_path = 'dataframes/scales_clean.csv'\n",
    "output_files_dir = 'data/outputs/'  # Directory containing batch output files\n",
    "#failed_output_files_dir = 'dataframes/tests/gpt40-mini/outputs/failed_outputs'  # Directory containing batch output files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sentences and items\n",
    "sentences = pd.read_csv(sentences_path)\n",
    "items = pd.read_csv(items_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    hmid                                         cleaned_hm  \\\n",
      "0  27673  I went on a successful date with someone I fel...   \n",
      "1  27674  I was happy when my son got 90% marks in his e...   \n",
      "2  27675       I went to the gym this morning and did yoga.   \n",
      "3  27676  We had a serious talk with some friends of our...   \n",
      "4  27677  I went with grandchildren to butterfly display...   \n",
      "\n",
      "  PERMA_Accomplishment_the_speaker_felt_they_were_making_progress_towards_accomplishing_their_goals?  \\\n",
      "0                                                NaN                                                   \n",
      "1                                                NaN                                                   \n",
      "2                                                NaN                                                   \n",
      "3                                                NaN                                                   \n",
      "4                                                NaN                                                   \n",
      "\n",
      "  PERMA_Engagement_the_speaker_became_absorbed_in_what_they_were_doing?  \\\n",
      "0                                                NaN                      \n",
      "1                                                NaN                      \n",
      "2                                                NaN                      \n",
      "3                                                NaN                      \n",
      "4                                                NaN                      \n",
      "\n",
      "  PERMA_Positive_Emotion_the_speaker_felt_joyful?  \\\n",
      "0                                             NaN   \n",
      "1                                             NaN   \n",
      "2                                             NaN   \n",
      "3                                             NaN   \n",
      "4                                             NaN   \n",
      "\n",
      "  PERMA_Negative_emotion_the_speaker_felt_anxious?  \\\n",
      "0                                              NaN   \n",
      "1                                              NaN   \n",
      "2                                              NaN   \n",
      "3                                              NaN   \n",
      "4                                              NaN   \n",
      "\n",
      "  PERMA_Accomplishment_the_speaker_achieved_the_important_goals_they_set_for_themselves?  \\\n",
      "0                                                NaN                                       \n",
      "1                                                NaN                                       \n",
      "2                                                NaN                                       \n",
      "3                                                NaN                                       \n",
      "4                                                NaN                                       \n",
      "\n",
      "  PERMA_Health_the_speaker_perceived_their_health_positively?  \\\n",
      "0                                                NaN            \n",
      "1                                                NaN            \n",
      "2                                                NaN            \n",
      "3                                                NaN            \n",
      "4                                                NaN            \n",
      "\n",
      "  PERMA_Meaning_the_speaker_felt_their_life_was_purposeful_and_meaningful?  \\\n",
      "0                                                NaN                         \n",
      "1                                                NaN                         \n",
      "2                                                NaN                         \n",
      "3                                                NaN                         \n",
      "4                                                NaN                         \n",
      "\n",
      "  PERMA_Relationships_the_speaker_received_help_and_support_from_others_when_needed?  \\\n",
      "0                                                NaN                                   \n",
      "1                                                NaN                                   \n",
      "2                                                NaN                                   \n",
      "3                                                NaN                                   \n",
      "4                                                NaN                                   \n",
      "\n",
      "   ...  \\\n",
      "0  ...   \n",
      "1  ...   \n",
      "2  ...   \n",
      "3  ...   \n",
      "4  ...   \n",
      "\n",
      "  CIT_Optimism_the_speaker_expected_more_good_things_in_their_life_than_bad?  \\\n",
      "0                                                NaN                           \n",
      "1                                                NaN                           \n",
      "2                                                NaN                           \n",
      "3                                                NaN                           \n",
      "4                                                NaN                           \n",
      "\n",
      "  CIT_Subjective_Well-Being_-_Life_Satisfaction_the_speaker_felt_in_most_ways_their_life_was_close_to_their_ideal?  \\\n",
      "0                                                NaN                                                                 \n",
      "1                                                NaN                                                                 \n",
      "2                                                NaN                                                                 \n",
      "3                                                NaN                                                                 \n",
      "4                                                NaN                                                                 \n",
      "\n",
      "  CIT_Subjective_Well-Being_-_Life_Satisfaction_the_speaker_felt_satisfied_with_their_life?  \\\n",
      "0                                                NaN                                          \n",
      "1                                                NaN                                          \n",
      "2                                                NaN                                          \n",
      "3                                                NaN                                          \n",
      "4                                                NaN                                          \n",
      "\n",
      "  CIT_Subjective_Well-Being_-_Life_Satisfaction_the_speaker_felt_their_life_is_going_well?  \\\n",
      "0                                                NaN                                         \n",
      "1                                                NaN                                         \n",
      "2                                                NaN                                         \n",
      "3                                                NaN                                         \n",
      "4                                                NaN                                         \n",
      "\n",
      "  CIT_Subjective_Well-Being_-_Positive_Feelings_the_speaker_felt_positive_most_of_the_time?  \\\n",
      "0                                                NaN                                          \n",
      "1                                                NaN                                          \n",
      "2                                                NaN                                          \n",
      "3                                                NaN                                          \n",
      "4                                                NaN                                          \n",
      "\n",
      "  CIT_Subjective_Well-Being_-_Positive_Feelings_the_speaker_felt_happy_most_of_the_time?  \\\n",
      "0                                                NaN                                       \n",
      "1                                                NaN                                       \n",
      "2                                                NaN                                       \n",
      "3                                                NaN                                       \n",
      "4                                                NaN                                       \n",
      "\n",
      "  CIT_Subjective_Well-Being_-_Positive_Feelings_the_speaker_felt_good_most_of_the_time?  \\\n",
      "0                                                NaN                                      \n",
      "1                                                NaN                                      \n",
      "2                                                NaN                                      \n",
      "3                                                NaN                                      \n",
      "4                                                NaN                                      \n",
      "\n",
      "  CIT_Subjective_Well-Being_-_Negative_Feelings_the_speaker_felt_negative_most_of_the_time?  \\\n",
      "0                                                NaN                                          \n",
      "1                                                NaN                                          \n",
      "2                                                NaN                                          \n",
      "3                                                NaN                                          \n",
      "4                                                NaN                                          \n",
      "\n",
      "  CIT_Subjective_Well-Being_-_Negative_Feelings_the_speaker_experienced_unhappy_feelings_most_of_the_time?  \\\n",
      "0                                                NaN                                                         \n",
      "1                                                NaN                                                         \n",
      "2                                                NaN                                                         \n",
      "3                                                NaN                                                         \n",
      "4                                                NaN                                                         \n",
      "\n",
      "  CIT_Subjective_Well-Being_-_Negative_Feelings_the_speaker_felt_bad_most_of_the_time?  \n",
      "0                                                NaN                                    \n",
      "1                                                NaN                                    \n",
      "2                                                NaN                                    \n",
      "3                                                NaN                                    \n",
      "4                                                NaN                                    \n",
      "\n",
      "[5 rows x 199 columns]\n",
      "Index(['hmid', 'cleaned_hm',\n",
      "       'PERMA_Accomplishment_the_speaker_felt_they_were_making_progress_towards_accomplishing_their_goals?',\n",
      "       'PERMA_Engagement_the_speaker_became_absorbed_in_what_they_were_doing?',\n",
      "       'PERMA_Positive_Emotion_the_speaker_felt_joyful?',\n",
      "       'PERMA_Negative_emotion_the_speaker_felt_anxious?',\n",
      "       'PERMA_Accomplishment_the_speaker_achieved_the_important_goals_they_set_for_themselves?',\n",
      "       'PERMA_Health_the_speaker_perceived_their_health_positively?',\n",
      "       'PERMA_Meaning_the_speaker_felt_their_life_was_purposeful_and_meaningful?',\n",
      "       'PERMA_Relationships_the_speaker_received_help_and_support_from_others_when_needed?',\n",
      "       ...\n",
      "       'CIT_Optimism_the_speaker_expected_more_good_things_in_their_life_than_bad?',\n",
      "       'CIT_Subjective_Well-Being_-_Life_Satisfaction_the_speaker_felt_in_most_ways_their_life_was_close_to_their_ideal?',\n",
      "       'CIT_Subjective_Well-Being_-_Life_Satisfaction_the_speaker_felt_satisfied_with_their_life?',\n",
      "       'CIT_Subjective_Well-Being_-_Life_Satisfaction_the_speaker_felt_their_life_is_going_well?',\n",
      "       'CIT_Subjective_Well-Being_-_Positive_Feelings_the_speaker_felt_positive_most_of_the_time?',\n",
      "       'CIT_Subjective_Well-Being_-_Positive_Feelings_the_speaker_felt_happy_most_of_the_time?',\n",
      "       'CIT_Subjective_Well-Being_-_Positive_Feelings_the_speaker_felt_good_most_of_the_time?',\n",
      "       'CIT_Subjective_Well-Being_-_Negative_Feelings_the_speaker_felt_negative_most_of_the_time?',\n",
      "       'CIT_Subjective_Well-Being_-_Negative_Feelings_the_speaker_experienced_unhappy_feelings_most_of_the_time?',\n",
      "       'CIT_Subjective_Well-Being_-_Negative_Feelings_the_speaker_felt_bad_most_of_the_time?'],\n",
      "      dtype='object', length=199)\n"
     ]
    }
   ],
   "source": [
    "# Clean and sanitize column names\n",
    "items['Scale'] = items['Scale'].str.strip().str.replace(r\"\\s+\", \"_\", regex=True)\n",
    "items['Dimension'] = items['Dimension'].str.strip().str.replace(r\"\\s+\", \"_\", regex=True)\n",
    "items['Items'] = items['Items'].str.strip().str.replace(r\"\\s+\", \"_\", regex=True)\n",
    "\n",
    "# Create flattened column names\n",
    "columns = [f\"{scale}_{dimension}_{item}\" for scale, dimension, item in zip(\n",
    "    items['Scale'], items['Dimension'], items['Items']\n",
    ")]\n",
    "\n",
    "# Initialize the ratings DataFrame\n",
    "ratings = pd.DataFrame(columns=[\"hmid\", \"cleaned_hm\"] + columns)\n",
    "ratings[\"hmid\"] = sentences[\"hmid\"]\n",
    "ratings[\"cleaned_hm\"] = sentences[\"cleaned_hm\"]\n",
    "\n",
    "print(ratings.head())\n",
    "print(ratings.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009\n"
     ]
    }
   ],
   "source": [
    "# Prepare a DataFrame to log failed responses\n",
    "failed_responses = []\n",
    "\n",
    "# Process all batch output files\n",
    "# Function to extract numeric value from a filename for proper sorting\n",
    "def extract_numeric(filepath):\n",
    "    filename = os.path.basename(filepath)  # Get the file name\n",
    "    match = re.search(r'\\d+', filename)   # Find the first numeric value\n",
    "    return int(match.group()) if match else float('inf')  # Return the number or infinity\n",
    "\n",
    "# Define directories in the specific processing order\n",
    "directories = [\n",
    "    output_files_dir,\n",
    "]\n",
    "\n",
    "# Collect files in the correct order and sort them numerically within each directory\n",
    "output_files = []\n",
    "for directory in directories:\n",
    "    files_in_dir = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(\".jsonl\")]\n",
    "    files_in_dir.sort(key=extract_numeric)  # Sort numerically within the directory\n",
    "    output_files.extend(files_in_dir)\n",
    "\n",
    "print(len(output_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing output file: data/outputs/output_log_batch_requests_1.jsonl\n",
      "Processing output file: data/outputs/output_log_batch_requests_2.jsonl\n",
      "Processing output file: data/outputs/output_log_batch_requests_3.jsonl\n",
      "Processing output file: data/outputs/output_log_batch_requests_4.jsonl\n",
      "Processing output file: data/outputs/output_log_batch_requests_5.jsonl\n",
      "Processing output file: data/outputs/output_log_batch_requests_6.jsonl\n",
      "Processing output file: data/outputs/output_log_batch_requests_7.jsonl\n",
      "Processing output file: data/outputs/output_log_batch_requests_8.jsonl\n",
      "Processing output file: data/outputs/output_log_batch_requests_9.jsonl\n",
      "Processing output file: data/outputs/output_log_batch_requests_10.jsonl\n",
      "Processing output file: data/outputs/output_log_batch_requests_11.jsonl\n",
      "Processing output file: data/outputs/output_log_batch_requests_12.jsonl\n",
      "Processing output file: data/outputs/output_log_batch_requests_13.jsonl\n",
      "Processing output file: data/outputs/output_log_batch_requests_14.jsonl\n",
      "Processing output file: data/outputs/output_log_batch_requests_15.jsonl\n",
      "Processing output file: data/outputs/output_log_batch_requests_16.jsonl\n",
      "Processing output file: data/outputs/output_log_batch_requests_17.jsonl\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 19\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Extract IDs\u001B[39;00m\n\u001B[1;32m     18\u001B[0m sent_id, item_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmap\u001B[39m(\u001B[38;5;28mint\u001B[39m, custom_id\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;241m1\u001B[39m:])\n\u001B[0;32m---> 19\u001B[0m sentence \u001B[38;5;241m=\u001B[39m sentences\u001B[38;5;241m.\u001B[39mloc[\u001B[43msentences\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mhmid\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43msent_id\u001B[49m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcleaned_hm\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     20\u001B[0m item \u001B[38;5;241m=\u001B[39m items\u001B[38;5;241m.\u001B[39miloc[item_idx]\n\u001B[1;32m     21\u001B[0m scale, dimension, item_text \u001B[38;5;241m=\u001B[39m item[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mScale\u001B[39m\u001B[38;5;124m\"\u001B[39m], item[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDimension\u001B[39m\u001B[38;5;124m\"\u001B[39m], item[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mItems\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/Desktop/Github/happyDB/openaivenv/lib/python3.13/site-packages/pandas/core/ops/common.py:76\u001B[0m, in \u001B[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001B[0;34m(self, other)\u001B[0m\n\u001B[1;32m     72\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mNotImplemented\u001B[39m\n\u001B[1;32m     74\u001B[0m other \u001B[38;5;241m=\u001B[39m item_from_zerodim(other)\n\u001B[0;32m---> 76\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmethod\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mother\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Github/happyDB/openaivenv/lib/python3.13/site-packages/pandas/core/arraylike.py:40\u001B[0m, in \u001B[0;36mOpsMixin.__eq__\u001B[0;34m(self, other)\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;129m@unpack_zerodim_and_defer\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__eq__\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__eq__\u001B[39m(\u001B[38;5;28mself\u001B[39m, other):\n\u001B[0;32m---> 40\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cmp_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moperator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meq\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Github/happyDB/openaivenv/lib/python3.13/site-packages/pandas/core/series.py:6119\u001B[0m, in \u001B[0;36mSeries._cmp_method\u001B[0;34m(self, other, op)\u001B[0m\n\u001B[1;32m   6116\u001B[0m lvalues \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values\n\u001B[1;32m   6117\u001B[0m rvalues \u001B[38;5;241m=\u001B[39m extract_array(other, extract_numpy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, extract_range\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m-> 6119\u001B[0m res_values \u001B[38;5;241m=\u001B[39m \u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcomparison_op\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   6121\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_construct_result(res_values, name\u001B[38;5;241m=\u001B[39mres_name)\n",
      "File \u001B[0;32m~/Desktop/Github/happyDB/openaivenv/lib/python3.13/site-packages/pandas/core/ops/array_ops.py:347\u001B[0m, in \u001B[0;36mcomparison_op\u001B[0;34m(left, right, op)\u001B[0m\n\u001B[1;32m    344\u001B[0m     res_values \u001B[38;5;241m=\u001B[39m comp_method_OBJECT_ARRAY(op, lvalues, rvalues)\n\u001B[1;32m    346\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 347\u001B[0m     res_values \u001B[38;5;241m=\u001B[39m \u001B[43m_na_arithmetic_op\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_cmp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    349\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m res_values\n",
      "File \u001B[0;32m~/Desktop/Github/happyDB/openaivenv/lib/python3.13/site-packages/pandas/core/ops/array_ops.py:189\u001B[0m, in \u001B[0;36m_na_arithmetic_op\u001B[0;34m(left, right, op, is_cmp)\u001B[0m\n\u001B[1;32m    185\u001B[0m     result \u001B[38;5;241m=\u001B[39m result\u001B[38;5;241m.\u001B[39mreshape(x\u001B[38;5;241m.\u001B[39mshape)  \u001B[38;5;66;03m# 2D compat\u001B[39;00m\n\u001B[1;32m    186\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n\u001B[0;32m--> 189\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_na_arithmetic_op\u001B[39m(left: np\u001B[38;5;241m.\u001B[39mndarray, right, op, is_cmp: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m    190\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    191\u001B[0m \u001B[38;5;124;03m    Return the result of evaluating op on the passed in values.\u001B[39;00m\n\u001B[1;32m    192\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    209\u001B[0m \u001B[38;5;124;03m    TypeError : invalid operation\u001B[39;00m\n\u001B[1;32m    210\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m    211\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(right, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    212\u001B[0m         \u001B[38;5;66;03m# can never use numexpr\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Prepare a DataFrame to log failed responses\n",
    "updates = []\n",
    "failed_responses = []\n",
    "\n",
    "for output_file in output_files:\n",
    "    print(f\"Processing output file: {output_file}\")\n",
    "    with open(output_file, 'r') as results_file:\n",
    "        for line in results_file:\n",
    "            response = json.loads(line)\n",
    "            try:\n",
    "                # Extract details\n",
    "                custom_id = response[\"custom_id\"]\n",
    "                result_body = response.get(\"response\", {}).get(\"body\", {})\n",
    "                choice = result_body.get(\"choices\", [{}])[0]\n",
    "                response_text = choice.get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "\n",
    "                # Extract IDs\n",
    "                sent_id, item_idx = map(int, custom_id.split(\"-\")[1:])\n",
    "                sentence = sentences.loc[sentences['hmid'] == sent_id, 'cleaned_hm'].values[0]\n",
    "                item = items.iloc[item_idx]\n",
    "                scale, dimension, item_text = item[\"Scale\"], item[\"Dimension\"], item[\"Items\"]\n",
    "                column_name = f\"{scale}_{dimension}_{item_text}\"\n",
    "\n",
    "                # Validate and add update\n",
    "                try:\n",
    "                    response_number = int(response_text.strip())\n",
    "                    updates.append((sent_id, column_name, response_number))\n",
    "                except ValueError:\n",
    "                    failed_responses.append({\n",
    "                        \"custom_id\": custom_id,\n",
    "                        \"hmid\": sent_id,\n",
    "                        \"sentence\": sentence,\n",
    "                        \"scale\": scale,\n",
    "                        \"dimension\": dimension,\n",
    "                        \"item\": item_text,\n",
    "                        \"response\": response_text\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing custom_id {custom_id}: {e}\")\n",
    "\n",
    "# Apply updates to the ratings DataFrame\n",
    "if updates:\n",
    "    # Convert updates to a DataFrame\n",
    "    updates_df = pd.DataFrame(updates, columns=[\"hmid\", \"column_name\", \"response_number\"])\n",
    "\n",
    "    # Reshape updates to have one column per column_name\n",
    "    updates_pivot = updates_df.pivot(index=\"hmid\", columns=\"column_name\", values=\"response_number\")\n",
    "\n",
    "    # Merge updates with the existing ratings DataFrame\n",
    "    ratings = ratings.set_index(\"hmid\")\n",
    "    ratings.update(updates_pivot)\n",
    "    ratings = ratings.reset_index()\n",
    "'''\n",
    "# Save ratings\n",
    "output_matrix_path = 'data/ratings_matrix.csv'\n",
    "ratings.to_csv(output_matrix_path, index=False)\n",
    "print(f\"Ratings matrix saved to {output_matrix_path}.\")\n",
    "\n",
    "# Save failed responses\n",
    "if failed_responses:\n",
    "    failed_responses_df = pd.DataFrame(failed_responses)\n",
    "    failed_responses_file = 'data/failure/failed_responses.csv'\n",
    "    failed_responses_df.to_csv(failed_responses_file, index=False)\n",
    "    print(f\"Failed responses logged to {failed_responses_file}.\")\n",
    "else:\n",
    "    print(\"No failed responses to log.\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_output_files_dir = 'data/failure/outputs/'  # Directory containing batch output files\n",
    "ratings = pd.read_csv('data/ratings_matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a DataFrame to log failed responses\n",
    "failed_responses = []\n",
    "\n",
    "\n",
    "failed_output_file_path = os.path.join('data/failure/outputs/failed_responses_4.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/failure/outputs/failed_responses_4.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(failed_output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing output file: data/failure/outputs/failed_responses_4.jsonl\n",
      "Line 1: sent_id=30250, item_idx=18, response_text='Please'\n",
      "Line 2: sent_id=33066, item_idx=149, response_text='Please'\n",
      "Line 3: sent_id=53671, item_idx=18, response_text='Please'\n",
      "Line 4: sent_id=63339, item_idx=2, response_text='Sure'\n",
      "Line 5: sent_id=63339, item_idx=3, response_text='Please'\n",
      "Line 6: sent_id=63339, item_idx=5, response_text='Please'\n",
      "Line 7: sent_id=63339, item_idx=12, response_text='Sure'\n",
      "Line 8: sent_id=63339, item_idx=15, response_text='Please'\n",
      "Line 9: sent_id=63339, item_idx=18, response_text='Sure'\n",
      "Line 10: sent_id=63339, item_idx=22, response_text='Please'\n",
      "Line 11: sent_id=63339, item_idx=48, response_text='Please'\n",
      "Line 12: sent_id=63339, item_idx=50, response_text='I'm'\n",
      "Line 13: sent_id=63339, item_idx=67, response_text='4'\n",
      "Line 14: sent_id=63339, item_idx=86, response_text='Please'\n",
      "Line 15: sent_id=63339, item_idx=128, response_text='Please'\n",
      "Line 16: sent_id=63339, item_idx=129, response_text='Please'\n",
      "Line 17: sent_id=63339, item_idx=136, response_text='Please'\n",
      "Line 18: sent_id=63339, item_idx=137, response_text='Please'\n",
      "Line 19: sent_id=63339, item_idx=140, response_text='Please'\n",
      "Line 20: sent_id=63339, item_idx=155, response_text='Please'\n",
      "Line 21: sent_id=63339, item_idx=159, response_text='Please'\n",
      "Line 22: sent_id=63339, item_idx=167, response_text='Please'\n",
      "Line 23: sent_id=68040, item_idx=2, response_text='4'\n",
      "Line 24: sent_id=68040, item_idx=3, response_text='4'\n",
      "Line 25: sent_id=68040, item_idx=12, response_text='Sure'\n",
      "Line 26: sent_id=68040, item_idx=18, response_text='Sure'\n",
      "Line 27: sent_id=68040, item_idx=22, response_text='Please'\n",
      "Line 28: sent_id=68040, item_idx=49, response_text='Please'\n",
      "Line 29: sent_id=68040, item_idx=159, response_text='Please'\n",
      "Line 30: sent_id=89917, item_idx=2, response_text='Please'\n",
      "Line 31: sent_id=89917, item_idx=3, response_text='Please'\n",
      "Line 32: sent_id=89917, item_idx=4, response_text='Please'\n",
      "Line 33: sent_id=89917, item_idx=12, response_text='Please'\n",
      "Line 34: sent_id=89917, item_idx=14, response_text='Please'\n",
      "Line 35: sent_id=89917, item_idx=15, response_text='Please'\n",
      "Line 36: sent_id=89917, item_idx=22, response_text='Please'\n",
      "Line 37: sent_id=89917, item_idx=34, response_text='Please'\n",
      "Line 38: sent_id=89917, item_idx=40, response_text='Please'\n",
      "Line 39: sent_id=89917, item_idx=46, response_text='Please'\n",
      "Line 40: sent_id=89917, item_idx=49, response_text='Please'\n",
      "Line 41: sent_id=89917, item_idx=50, response_text='Please'\n",
      "Line 42: sent_id=89917, item_idx=54, response_text='Please'\n",
      "Line 43: sent_id=89917, item_idx=73, response_text='Please'\n",
      "Line 44: sent_id=89917, item_idx=99, response_text='Please'\n",
      "Line 45: sent_id=89917, item_idx=119, response_text='Please'\n",
      "Line 46: sent_id=89917, item_idx=121, response_text='4'\n",
      "Line 47: sent_id=89917, item_idx=129, response_text='Please'\n",
      "Line 48: sent_id=89917, item_idx=131, response_text='Please'\n",
      "Line 49: sent_id=89917, item_idx=133, response_text='Please'\n",
      "Line 50: sent_id=89917, item_idx=136, response_text='Please'\n",
      "Line 51: sent_id=89917, item_idx=158, response_text='Please'\n",
      "Line 52: sent_id=89917, item_idx=159, response_text='3'\n",
      "Line 53: sent_id=89917, item_idx=165, response_text='Please'\n",
      "Line 54: sent_id=89917, item_idx=167, response_text='Please'\n",
      "Line 55: sent_id=89917, item_idx=184, response_text='4'\n",
      "Line 56: sent_id=97926, item_idx=12, response_text='Please'\n",
      "Line 57: sent_id=117786, item_idx=12, response_text='3'\n",
      "Line 58: sent_id=117786, item_idx=14, response_text='I'm'\n",
      "Line 59: sent_id=117786, item_idx=50, response_text='Please'\n",
      "Line 60: sent_id=117786, item_idx=137, response_text='Please'\n",
      "Line 61: sent_id=119269, item_idx=15, response_text='4'\n",
      "Line 62: sent_id=119269, item_idx=140, response_text='Please'\n",
      "Line 63: sent_id=127681, item_idx=3, response_text='Please'\n",
      "Line 64: sent_id=127681, item_idx=12, response_text='Please'\n",
      "Line 65: sent_id=127681, item_idx=14, response_text='Please'\n",
      "Line 66: sent_id=127681, item_idx=50, response_text='4'\n",
      "Line 67: sent_id=127681, item_idx=149, response_text='Please'\n",
      "Line 68: sent_id=127681, item_idx=158, response_text='Please'\n",
      "Ratings matrix saved to data/ratings_matrix.csv.\n",
      "Failed responses logged to data/failure/failed_responses5.csv.\n"
     ]
    }
   ],
   "source": [
    "# Parse files and collect updates\n",
    "updates = []\n",
    "failed_responses = []\n",
    "\n",
    "\n",
    "print(f\"Processing output file: {failed_output_file_path}\")\n",
    "with open(failed_output_file_path, 'r') as results_file:\n",
    "    for line_number, line in enumerate(results_file, start=1):\n",
    "        try:\n",
    "            response = json.loads(line.strip())\n",
    "\n",
    "            # Extract `custom_id`\n",
    "            custom_id = response.get(\"custom_id\", \"\")\n",
    "            if not custom_id or \"-\" not in custom_id:\n",
    "                print(f\"Invalid or missing `custom_id` on line {line_number}: {custom_id}\")\n",
    "                continue\n",
    "\n",
    "            # Extract IDs\n",
    "            try:\n",
    "                sent_id, item_idx = map(int, custom_id.split(\"-\")[1:])\n",
    "            except ValueError as e:\n",
    "                print(f\"Error parsing `custom_id` on line {line_number}: {custom_id}\")\n",
    "                continue\n",
    "\n",
    "            # Extract `response_text`\n",
    "            try:\n",
    "                choice = response[\"response\"][\"choices\"][0]\n",
    "                response_text = choice.get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "                if not response_text:\n",
    "                    print(f\"Empty `response_text` on line {line_number}\")\n",
    "                    raise ValueError(\"Empty response text\")\n",
    "            except KeyError as e:\n",
    "                print(f\"Invalid structure for response on line {line_number}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Log debug details for this entry\n",
    "            print(f\"Line {line_number}: sent_id={sent_id}, item_idx={item_idx}, response_text='{response_text}'\")\n",
    "\n",
    "            # Skip if item_idx is out of bounds for `items`\n",
    "            if item_idx >= len(items):\n",
    "                print(f\"Skipping line {line_number}: item_idx {item_idx} is out of bounds.\")\n",
    "                continue\n",
    "\n",
    "            # Extract sentence and item details\n",
    "            try:\n",
    "                sentence = sentences.loc[sentences['hmid'] == sent_id, 'cleaned_hm'].values[0]\n",
    "                item = items.iloc[item_idx]\n",
    "                scale, dimension, item_text = item[\"Scale\"], item[\"Dimension\"], item[\"Items\"]\n",
    "                column_name = f\"{scale}_{dimension}_{item_text}\"\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to match `sent_id` or `item_idx` on line {line_number}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Validate and add update\n",
    "            try:\n",
    "                response_number = int(response_text)\n",
    "                updates.append((sent_id, column_name, response_number))\n",
    "            except ValueError:\n",
    "                failed_responses.append({\n",
    "                    \"custom_id\": custom_id,\n",
    "                    \"hmid\": sent_id,\n",
    "                    \"sentence\": sentence,\n",
    "                    \"scale\": scale,\n",
    "                    \"dimension\": dimension,\n",
    "                    \"item\": item_text,\n",
    "                    \"response\": response_text\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing line {line_number}: {e}\")\n",
    "\n",
    "# Apply updates to the ratings DataFrame\n",
    "if updates:\n",
    "    # Convert updates to a DataFrame\n",
    "    updates_df = pd.DataFrame(updates, columns=[\"hmid\", \"column_name\", \"response_number\"])\n",
    "\n",
    "    # Reshape updates to have one column per column_name\n",
    "    updates_pivot = updates_df.pivot(index=\"hmid\", columns=\"column_name\", values=\"response_number\")\n",
    "\n",
    "    # Merge updates with the existing ratings DataFrame\n",
    "    ratings = ratings.set_index(\"hmid\")\n",
    "    ratings.update(updates_pivot)\n",
    "    ratings = ratings.reset_index()\n",
    "\n",
    "# Save ratings\n",
    "output_matrix_path = 'data/ratings_matrix.csv'\n",
    "ratings.to_csv(output_matrix_path, index=False)\n",
    "print(f\"Ratings matrix saved to {output_matrix_path}.\")\n",
    "\n",
    "# Save failed responses\n",
    "if failed_responses:\n",
    "    failed_responses_df = pd.DataFrame(failed_responses)\n",
    "    failed_responses_file = 'data/failure/failed_responses5.csv'\n",
    "    failed_responses_df.to_csv(failed_responses_file, index=False)\n",
    "    print(f\"Failed responses logged to {failed_responses_file}.\")\n",
    "else:\n",
    "    print(\"No failed responses to log.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total empty slots (NaN values) in the matrix: 59\n"
     ]
    }
   ],
   "source": [
    "# Count total empty (NaN) slots\n",
    "empty_slots = ratings.isna().sum().sum()\n",
    "\n",
    "print(f\"Total empty slots (NaN values) in the matrix: {empty_slots}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings matrix saved to dataframes/tests/gpt40-mini/ratings_matrix.csv.\n"
     ]
    }
   ],
   "source": [
    "# Save the ratings matrix as a CSV file\n",
    "output_matrix_path = 'data/ratings_matrix.csv'\n",
    "ratings.to_csv(output_matrix_path)\n",
    "print(f\"Ratings matrix saved to {output_matrix_path}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openaivenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
